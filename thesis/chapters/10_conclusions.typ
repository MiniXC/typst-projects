== Conclusions and Limitations <10_conclusions>

This thesis  systematically investigated the persistent disparity between the perceived naturalness of synthetic speech and its practical utility for downstream applications, specifically Automatic Speech Recognition (ASR) training. By introducing quantitative methodologies such as the Word Error Rate Ratio (WERR) and the Text-to-Speech Distribution Score (TTSDS), we have provided novel insights into the nuanced differences between real and synthetic speech distributions. The core motivation of this work stemmed from the observation that despite Text-to-Speech (TTS) systems achieving human-level naturalness in subjective listening tests, synthetic speech consistently underperforms when used to train ASR models. Our findings confirm this initial premise and offer a partial explanations for this #emph[synthetic-real gap].

The initial objective of explaining the observed gap in ASR performance between models trained on real versus synthetic data was addressed by adopting a controlled experimental setup, as detailed in @05_ttsasr. The introduction of the WERR metric provided a direct and quantifiable measure of this disparity, consistently showing that ASR models trained solely on synthetic speech exhibit significantly higher error rates (often by a factor of 2 or more) compared to those trained on real data. This empirical evidence challenges the implicit assumption that high subjective naturalness directly translates to utility for robust ASR training. Our cross-evaluation experiments, where ASR models trained on real data surprisingly performed well on synthetic data, further reinforced the notion that evenly natural-sounding synthetic speech, seems to be more regular does not fully capture the complexity of real human speech on a distributional level.

To mitigate this gap, we explored various methods aimed at enhancing the diversity of synthetic speech, ranging from explicit attribute conditioning to post-generation augmentation, as detailed in @06_attr. Our controlled experiments demonstrated that explicit conditioning on measurable attributes—such as mean pitch, energy, and duration, as well as environmental correlates like Speech-to-Reverberation Modulation Energy Ratio (SRMR) and Waveform Amplitude Distribution Analysis Signal-to-Noise Ratio (WADA SNR) provided improvements. Especially post-generation data augmentation, by introducing realistic background noise and reverberation, yielded a substantial reduction in WERR, showing the importance of diverse acoustic environments for ASR training. However, even with the #emph[Oracle System] which utilized ground-truth attributes for conditioning, a considerable gap to real data remained, suggesting inherent limitations in the TTS model's ability to fully capture the real speech distribution. These findings underscored that while diversity enhancements are beneficial, they do not fully resolve the underlying distributional discrepancy.

The investigation into scaling properties in @07_scaling further quantified the specifics of closing the synthetic-real gap. We demonstrated distinct scaling behaviors for synthetic data generated by Mean Squared Error (MSE)-based and Denoising Diffusion Probabilistic Model (DDPM)-based TTS models. While MSE models outperformed DDPM in low-data regimes, their performance quickly plateaued due to oversmoothing. In contrast, DDPM models, with their inherent stochasticity and capacity to capture complex distributions, exhibited superior scalability, showing continuous improvements in WERR as the TTS training dataset size increased. Our proposed two-term power law model, fit to the observed scaling behavior, indicated that even with the improved scaling of DDPMs, an extraordinary amount of synthetic data—on the order of millions of hours—would be required to match the performance of ASR systems trained on real data. This finding highlights that while scaling is a powerful strategy, it alone is unlikely to fully eliminate the gap.

Recognizing the limitations of task-specific metrics like WERR and the inherent subjectivity of human evaluation, a significant contribution of this thesis is the development and validation of the Text-to-Speech Distribution Score (TTSDS) in @09_dist. By conceptualizing speech as a distribution and employing the 2-Wasserstein distance (Earth Mover's Distance) to quantify dissimilarity, TTSDS offers a robust, objective, and factorized evaluation framework. TTSDS measures distances across perceptually motivated factors -- providing interpretable insights into which specific aspects of synthetic speech deviate from their real counterparts. We demonstrated that TTSDS, in both its initial (TTSDS1) and enhanced (TTSDS2) iterations, correlates strongly and robustly with human subjective judgments (MOS, CMOS, SMOS) across a wide range of TTS systems, temporal periods (2008-2024), diverse domains (clean, noisy, wild, kids), and multiple languages. This robust correlation validates TTSDS as a reliable objective metric that can predict human perception, identify specific areas for TTS improvement, and serve as a consistent benchmark for future TTS improvement, addressing the need for generalizable evaluation tools in an evolving field.

To summarise, this thesis has provided a comprehensive investigation into the synthetic-real gap in speech technology. We have quantified this gap, explored methods to reduce it, and developed a robust objective metric to measure it. The findings in this work consistently indicate that a fundamental distributional difference persists -- achieving true parity with real speech for ASR training remains a monumental challenge, likely either requiring orders of magnitude more data or novel architectural approaches.

=== Limitations

While this thesis provides an exploration of the synthetic-real gap in speech technology and introduces robust methodologies for its measurement, several limitations inherent in the scope of this work and the nature of the field should be acknowledged. Firstly, the primary focus on specific TTS architectures, such as FastSpeech 2 for diversity enhancements and a two-U-Net model for scaling experiments, means that the generalizability of some findings might be constrained. While these models represent widely adopted paradigms at the time of the research, the field of TTS is rapidly evolving with the emergence of new architectures, including large language model-based systems and those leveraging discrete audio tokens in novel ways. The observed scaling behaviors and effectiveness of diversity enhancement techniques might differ for these newer, potentially more powerful TTS systems.

Secondly, the ASR systems employed in this work, while representing strong and widely used baselines (hybrid HMM-TDNN and Conformer-CTC models), do not encompass the entire spectrum of state-of-the-art ASR architectures. The choice of ASR model, its specific configuration, and its training data can influence the measured WERR values. Although efforts were made to keep the ASR setup consistent to isolate the impact of synthetic data, it is possible that different ASR architectures might exhibit varying sensitivities to the particular characteristics of synthetic speech.

Thirdly, the metrics themselves, while robust, have inherent limitations. The WERR, serves as a task-specific heuristic for dissimilarity rather than a true statistical distance metric in a rigorous mathematical sense. Its values are influenced by the performance of the underlying ASR system. Similarly, TTSDS relies on the assumption that underlying speech embedding distributions can be approximated by multivariate Gaussians for some of its components. While this is a common and often reasonable assumption for learned latent spaces, its absolute validity for all speech features and all conditions cannot be universally guaranteed.

Finally, while large audiobook datasets like LibriHeavy and in-the-wild YouTube data as well as childrens speech were evaluated, real-world human speech contains an almost infinite variety of accents, dialects, speaking styles (e.g., whispered, shouted, singing), emotional expressions, and pathological speech. Even with large-scale training, the current TTS models and the synthetic data they produce may not fully capture this vast diversity, and any evaluation will have be biased as it is impossible to include all variations and forms of speech. The ethical considerations around generating synthetic speech, particularly in the context of voice cloning and the potential for misuse (e.g., deepfakes, misinformation), are ongoing societal concerns that extend beyond the technical scope of this thesis. While our methodology explicitly aimed to prevent the creation of novel content attributed to original speakers, the increasing realism of synthetic voices necessitates vigilance and responsible development within the broader research community.