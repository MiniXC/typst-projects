#import "../changes.typ": * 

== Conclusions <10_conclusions>

This thesis systematically investigated the persistent disparity between the perceived naturalness of synthetic speech and its practical utility for downstream applications, specifically Automatic Speech Recognition (ASR) training. By introducing quantitative methodologies such as the Word Error Rate Ratio (WERR) and the Text-to-Speech Distribution Score (TTSDS), we have provided novel insights into the nuanced differences between real and synthetic speech distributions. The core motivation of this work stemmed from the observation that despite Text-to-Speech (TTS) systems achieving human-level naturalness in subjective listening tests, synthetic speech consistently underperforms when used to train ASR models. Our findings confirm this initial premise and offer a partial explanations for this #emph[synthetic-real gap].

#minorchange[Wording][#strike["significant"]][The initial objective of explaining the observed gap in ASR performance between models trained on real versus synthetic data was addressed by adopting a controlled experimental setup, as detailed in @05_ttsasr. The introduction of the WERR metric provided a direct and quantifiable measure of this disparity, consistently showing that ASR models trained solely on synthetic speech exhibit higher error rates (often by a factor of 2 or more) compared to those trained on real data. This empirical evidence challenges the implicit assumption that high subjective naturalness directly translates to utility for robust ASR training. Our cross-evaluation experiments, where ASR models trained on real data surprisingly performed well on synthetic data, further reinforced the notion that even natural-sounding synthetic speech, which seems to be more regular, does not fully capture the complexity of real human speech on a distributional level.]

To mitigate this gap, we explored various methods aimed at enhancing the diversity of synthetic speech, ranging from explicit attribute conditioning to post-generation augmentation, as detailed in @06_diversity. Our controlled experiments demonstrated that explicit conditioning on measurable attributes -- such as mean pitch, energy, and duration, as well as environmental correlates like Speech-to-Reverberation Modulation Energy Ratio (SRMR) and Waveform Amplitude Distribution Analysis Signal-to-Noise Ratio (WADA SNR) provided improvements. Especially post-generation data augmentation, by introducing realistic background noise and reverberation, yielded a substantial reduction in WERR, showing the importance of diverse acoustic environments for ASR training. However, even with the #emph[Oracle System] which utilised ground-truth attributes for conditioning, a considerable gap to real data remained, suggesting inherent limitations in the TTS model's ability to fully capture the real speech distribution. These findings underscored that while diversity enhancements are beneficial, they do not fully resolve the underlying distributional discrepancy.

The investigation into scaling properties in @07_scaling further quantified the specifics of closing the synthetic-real gap. We demonstrated distinct scaling behaviours for synthetic data generated by Mean Squared Error (MSE)-based and Denoising Diffusion Probabilistic Model (DDPM)-based TTS models. While MSE models outperformed DDPM in low-data regimes, their performance quickly plateaued due to oversmoothing. In contrast, DDPM models, with their inherent stochasticity and capacity to capture complex distributions, exhibited superior scalability, showing continuous improvements in WERR as the TTS training dataset size increased. Our proposed two-term power law model, fit to the observed scaling behaviour, indicated that even with the improved scaling of DDPMs, a very large amount of synthetic data -- on the order of millions of hours -- would be required to match the performance of ASR systems trained on real data. This finding highlights that while scaling is a powerful strategy, it alone is unlikely to fully eliminate the gap.

#minorchange[Wording][#strike["significant"]][Recognising the limitations of task-specific metrics like WERR and the inherent subjectivity of human evaluation, an important contribution of this thesis is the development and validation of the Text-to-Speech Distribution Score (TTSDS) in @09_dist. By conceptualising speech as a distribution and employing the 2-Wasserstein distance (Earth Mover's Distance) to quantify dissimilarity, TTSDS offers a robust, objective, and factorised evaluation framework.] TTSDS measures distances across perceptually motivated factors -- providing interpretable insights into which specific aspects of synthetic speech deviate from their real counterparts. We demonstrated that TTSDS, in both its initial (TTSDS1) and enhanced (TTSDS2) iterations, correlates strongly and robustly with human subjective judgments (MOS, CMOS, SMOS) across a wide range of TTS systems, temporal periods (2008-2024), diverse domains (clean, noisy, wild, kids), and multiple languages. This robust correlation validates TTSDS as a reliable objective metric that can predict human perception, identify specific areas for TTS improvement, and serve as a consistent benchmark for future TTS improvement, addressing the need for generalisable evaluation tools in an evolving field.

#minorchange[Wording][#strike["significant"]][To summarise, this thesis has provided a comprehensive investigation into the synthetic-real gap in speech technology. We have quantified this gap, explored methods to reduce it, and developed a robust objective metric to measure it. The findings in this work consistently indicate that a fundamental distributional difference persists -- achieving true parity with real speech for ASR training remains a challenge, likely either requiring orders of magnitude more data or novel architectural approaches.]

=== Limitations

While this thesis provides an exploration of the synthetic-real gap in speech technology and introduces robust methodologies for its measurement, several limitations inherent in the scope of this work and the nature of the field should be acknowledged. Firstly, the primary focus on specific TTS architectures, such as FastSpeech 2 for diversity enhancements and a two-U-Net model for scaling experiments, means that the generalisability of some findings might be constrained. While these models represent widely adopted paradigms at the time of the research, the field of TTS is rapidly evolving with the emergence of new architectures, including large language model-based systems and those leveraging discrete audio tokens in novel ways. The observed scaling behaviours and effectiveness of diversity enhancement techniques might differ for these newer, potentially more powerful TTS systems.

Secondly, the ASR systems employed in this work, while representing strong and widely used baselines (hybrid HMM-TDNN and Conformer-CTC models), do not encompass the entire spectrum of state-of-the-art ASR architectures. The choice of ASR model, its specific configuration, and its training data can influence the measured WERR values. Although efforts were made to keep the ASR setup consistent to isolate the impact of synthetic data, it is possible that different ASR architectures might exhibit varying sensitivities to the particular characteristics of synthetic speech.

Thirdly, the metrics themselves, while robust, have inherent limitations. The WERR, serves as a task-specific heuristic for dissimilarity rather than a true statistical distance metric in a rigorous mathematical sense. Its values are influenced by the performance of the underlying ASR system. Similarly, TTSDS relies on the assumption that underlying speech embedding distributions can be approximated by multivariate Gaussians for some of its components. While this is a common and often reasonable assumption for learned latent spaces, its absolute validity for all speech features and all conditions cannot be universally guaranteed.

Finally, while large audiobook datasets like LibriHeavy and in-the-wild YouTube data as well as children's speech were evaluated, real-world human speech contains an almost infinite variety of accents, dialects, speaking styles (e.g., whispered, shouted, singing), emotional expressions, and pathological speech. Even with large-scale training, the current TTS models and the synthetic data they produce may not fully capture this vast diversity, and any evaluation will have be to biased as it is impossible to include all variations and forms of speech. The ethical considerations around generating synthetic speech, particularly in the context of voice cloning and the potential for misuse (e.g., deepfakes, misinformation), are ongoing societal concerns that extend beyond the technical scope of this thesis. While our methodology explicitly aimed to prevent the creation of novel content attributed to original speakers, the increasing realism of synthetic voices necessitates vigilance and responsible development within the broader research community.

=== Future Work

The findings and methodologies presented in this thesis open up several promising avenues for future research. The established synthetic-real gap and the tools developed to measure it -- WERR and TTSDS -- provide a foundation for further exploration into the nature of synthetic speech and its applications.

One promising direction involves leveraging TTSDS to enhance the efficiency and focus of subjective listening tests. Given that comprehensive subjective evaluations are resource-intensive, TTSDS could be employed as a pre-selection tool. Rather than conducting exhaustive pairwise comparisons across all available systems, researchers could use TTSDS and its factor scores to identify a smaller, more informative subset of systems for human evaluation. For example, listening tests could focus on systems that TTSDS finds to be distributionally close, to probe the limits of human perception, or on systems that exhibit interesting discrepancies in their factor scores (e.g., high #smallcaps[Prosody] but low #smallcaps[Speaker] realism). This approach would reduce the combinatorial complexity of subjective testing, allowing for more targeted and cost-effective human evaluation.

#minorchange[Wording][#strike["significant"]][An investigation into the relationship between the two primary metrics developed in this thesis, WERR and TTSDS, constitutes another avenue for future work. WERR measures the task-specific, downstream utility of synthetic speech for ASR training, while TTSDS quantifies its task-agnostic, perceptual distributional similarity to real speech. A systematic study exploring the correlation between these two metrics would be highly valuable. If a strong correlation were established, it would imply that the perceptual qualities captured by TTSDS are directly linked to the acoustic variability required for robust ASR. This would enable the use of the more efficient, reference-free TTSDS framework as a reliable proxy for predicting the utility of a synthetic dataset for ASR training, bridging the gap between perceptual evaluation and downstream task performance without the need to train full ASR models.]

Furthermore, the foundational work on TTSDS2, particularly its automated data collection and evaluation pipeline, provides a clear path for expansion. The framework can be extended to include a wider range of under-represented languages beyond the initial 14, continually improving its cross-lingual robustness. Similarly, the metric's applicability could be tested and refined on more challenging and diverse acoustic domains that were not covered in this thesis, such as highly emotional speech, whispered or shouted speech, singing voice synthesis, and pathological speech. Extending the evaluation to these domains would not only test the limits of the TTSDS framework but also provide crucial guidance for the development of TTS systems capable of handling the full breadth of human vocal expression.

Finally, a further area of potential application lies in the field of synthetic speech detection, commonly referred to as deepfake detection. While TTSDS is not designed for the classification of individual utterances, its ability to characterise distributions could be adapted for system-level forensic analysis. The factorised scores of TTSDS could serve as a "distributional fingerprint" for a given generative model. If a large corpus of suspicious audio samples were collected, TTSDS could analyse their collective properties. Consistent patterns -- such as an unusually low variance in pitch combined with high speaker similarity -- might point to a common synthetic origin and could even be used to attribute the samples to a specific class of TTS architecture. This approach could provide a valuable tool for researchers and platforms seeking to identify and understand large-scale, coordinated misinformation campaigns that leverage a single generative source.