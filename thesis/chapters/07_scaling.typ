#import "../comic.typ"
#import "../abbr.typ"
#import "../quote.typ": *
#import "../math.typ": *
#import "../moremath.typ": *
#import "@preview/booktabs:0.0.4": *
#show: booktabs-default-table-style

== Scaling properties for TTS-for-ASR <07_scaling>

#q(
  [#citep(<sutton_bitter_2019>)],
  [#emph[The Bitter Lesson]],
  [The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin.]
)

Consistent improvements in the performance of deep learning models with increasing computational resources and data has been observed. This phenomenon is formally described by neural scaling laws, which introduce a power-law relationship between a model's test error and factors such as the number of model parameters, the allocated compute budget, and, most pertinent to this work, the size of the training dataset @kaplan_scaling_2020@bahri_scaling_2024. These laws have been verified across a diverse range of machine learning domains, encompassing natural language processing @kaplan_scaling_2020, computer vision @fan_images_2024, and spoken language modeling @cuervo_language_2024.

In the field of computer vision, scaling studies on synthetic data for model training have been conducted @fan_images_2024. It has been demonstrated that while synthetic images generated by state-of-the-art text-to-image models can exhibit a scaling trend similar to real images for training vision models, their effectiveness is often reduced. Specifically, for training #abbr.a[CLIP] @radford_clip_2021 models with language supervision, synthetic images scale similarly to real images, albeit with slightly less effectiveness. However, for training supervised image classifiers with label supervision, synthetic images significantly underperform when scaled. The primary reasons identified for this underperformance include the inability of off-the-shelf text-to-image models to generate certain concepts accurately and a lack of diversity, which substantially impairs the training of image classifiers @fan_images_2024, similarly to our findings in the prior chapter. Our contributions in this chapter represent the first comprehensive study into the scaling behavior of synthetic data specifically for model training in the speech domain.

As #abbr.a[TTS] models continue to advance and are increasingly trained on ever-larger speech corpora @casanova_xtts_2024@chen_vall-e_2024, their capability to generate large quantities of high-quality synthetic speech expands proportionally. In this chapter, we investigate how the choice of #abbr.a[TTS] training objective impacts the scalability of synthetic data for #abbr.a[ASR] training, specifically comparing the widely used #abbr.a[MSE] against #abbr.pla[DDPM].

=== Neural Scaling Laws in Speech Recognition

For a wide spectrum of machine learning tasks, the relationship between the test error and the size of the training dataset ($D$) can be accurately modeled by a power-law function. In the domain of speech recognition, this scaling behavior has been empirically validated for discriminative #abbr.a[ASR] rescoring models @gu_discriminativespeech_2023. Under the assumption of a sufficiently large model capacity and computational budget, the #abbr.a[WER] of an #abbr.a[ASR] system is primarily limited by the quantity of training data. This relationship can be expressed formally as:

$
text("WER")(D) = (D_c/D)^alpha
$

where $D_c$ represents a characteristic dataset size (a positive constant specific to the task) and $alpha$ is a positive constant are empirically determined and model the rate of performance improvement with increasing data. This implies a consistent and predictable improvement in performance as more training data is provided, assuming the data distribution remains fixed and representative of the target task.

=== Challenges in Scaling with Synthetic Data

While the standard neural scaling law provides a framework for modeling performance when training on real data, its direct application to the #abbr.a[TTS]-for-#abbr.a[ASR] task introduces additional challenges. In this paradigm, the training data for the #abbr.a[ASR] system is entirely synthetic, which leads to different distributions of training and testing data, where the training data depends on the generative #abbr.a[TTS] model used to produce them and the testing data represents the real distribution.

==== Mean Squared Error (MSE) and Oversmoothing

As previously discussed in @03_tts[Chapter], #abbr.a[TTS] models trained to minimize Mean Squared Error (#abbr.a[MSE]) assume that the conditional probability distribution of the target speech feature (e.g., Mel spectrograms) given the input text is unimodal. Given that speech generation is an inherently "one-to-many" problem -- where a single text can correspond to many valid acoustic realizations -- this unimodal assumption leads to the well-documented issue of oversmoothing @ren_revisiting_2022. The model, when faced with multiple plausible outputs for a given input, learns to predict the statistical average of these possibilities. The resulting synthetic speech often exhibits low spectral variance and lacks fine-grained features present in real speech, contributing to the distributional gap highlighted in @05_ttsasr[Chapter].

==== Diffusion and Stochasticity

In contrast to #abbr.a[MSE]-based models, #abbr.pla[DDPM] are explicitly designed to model the entire, potentially multi-modal, data distribution by learning to reverse a stochastic noising process @ho_denoising_2020. This class of generative models operates through two main phases: a forward diffusion process and a reverse denoising process.

The #emph[forward diffusion process] involves gradually adding Gaussian noise to a clean data sample ($x_0$, which for #abbr.a[TTS] is usually a Mel spectrogram) over a series of $K$ timesteps. Each step slightly degrades the sample until, at timestep $K$, the data is transformed into pure Gaussian noise. This process is typically defined such that the distribution of $x_k$ given $x_0$ can be directly sampled:
$ q(x_k | x_(k-1)) = cal(N)(x_k; sqrt(1 - beta_k)x_(k-1),beta_k bold(I)) $
where $beta_k$ is a decreasing sequence of values that control the amount of signal retained at each timestep called the variance schedule. This means that as $k$ increases, $beta_k$ approaches zero, and $x_k$ becomes increasingly noisy.

The #emph[reverse denoising process] is where the model learns to invert the forward process, gradually removing the noise at each step to reconstruct the original data sample. This is achieved by training a neural network to predict the noise that was added at each timestep, predicting $x_(k-1)$ iteratively from $x_k$ until $k=0$. Thus the model, parametrized by $theta$, learns the conditional probability $p_theta (x_(k-1) | x_k)$, effectively reversing the noise addition. If $epsilon$ represents the true noise added to $x_(k-1)$ to get $x_k$, and $epsilon_theta(x_k, k)$ is the model's prediction of this noise, the objective is:
$ cal(L) = EE_(x_0, k, epsilon) [ |epsilon - epsilon_theta (sqrt(beta_k) x_0 + sqrt(1 - beta_k) epsilon, k)|^2_2] $
The inherent stochasticity in this process should, in theory, allow #abbr.pla[DDPM]-based #abbr.a[TTS] models to generate more diverse and representative synthetic speech, thus better approximating the complexity of real human speech.

==== Challenges when using DDPMs
A common challenge recently identified in the training of #abbr.pla[DDPM], including those used in text-to-image generation, concerns common noise schedules and sample steps @lin_common_2024. Many existing diffusion noise schedules do not enforce a zero #abbr.a[SNR] at the last timestep ($K$). The signal-to-noise ratio at timestep $k$ is given by 
$
"SNR"(k) = beta_k / (1 - beta_k)
$

A non-zero terminal #abbr.a[SNR] creates a fundamental discrepancy between the training and inference phases: during training, the model is exposed to a small amount of residual signal information even at the highest noise level ($x_K$), whereas during inference, it is typically given pure Gaussian noise with zero mean and no signal content. This leads to real problems in practice, such as limiting the generated samples to a plain, medium brightness range, or causing the model to fail in generating very bright or dark samples, up to not generating realistic samples at all.

To address these issues, several simple fixes have been proposed by #citep(<lin_common_2024>) of which we apply the following in our work: 1) rescaling the noise schedule to ensure zero terminal #abbr.a[SNR] (i.e., $bar(alpha)_K$ becomes 0), which ensures that $x_K$ is pure noise and consistent with inference input; 2) ensuring the sampler always starts from the last timestep ($K$) during inference, which aligns the inference process with the training expectation of pure noise input. These corrections ensure matching results between training and inference, allowing the model to generate samples that are faithful to the original data distribution.

The inherent stochasticity of #abbr.pla[DDPM], coupled with these fixes to their training and sampling procedures, makes them theoretically more capable of capturing the diverse acoustic realizations inherent in speech. However, their effectiveness in practice, and particularly their scalability for #abbr.a[ASR] training, still depends on sufficient training data and robust implementation.

=== Comparing MSE and DDPM-based training

Here, we aim to quantify the distinct scaling properties of synthetic data when generated by #abbr.pla[DDPM] versus #abbr.a[MSE]-based #abbr.a[TTS] models. Our primary focus is to evaluate how #abbr.a[ASR] performance, quantified by the #abbr.s[WERR], evolves with increasing #abbr.a[TTS] training dataset size and speaker diversity.

The core #abbr.a[TTS] architecture employed in our experiments consists of two U-Net models, as shown in @fig_tts_arch_scaling:
A U-Net Encoder ($"U-Net"_"ENC"$) is responsible for generating a two-dimensional representation of prosody. This model leverages the #abbr.a[CWT] of pitch and energy, similar to FastSpeech @ren_fastspeech_2019, and additionally includes the #abbr.a[CWT] of phone duration. This prosody representation $P$ is conditioned on the input phone sequence $"G2P"(T)$ and speaker identity represented by a d-vector $Z_"SPK"$:
    $ P = "U-Net"_"ENC" ("G2P"(T), Z_"SPK") $
    This multi-resolution analysis is inspired by FastSpeech 2 @ren_fastspeech_2021 and captures both temporal and frequency-domain characteristics.

    P is then expanded by deriving the per-phone duration values using the inverse CWT, such that $P'$ is $P$ repeated along the time axis according to said predicted durations.
    A second U-Net Decoder ($"U-Net"_"DEC"$) then transforms this prosody representation into a synthetic Mel spectrogram $cal(R)_"MEL" (tilde(S))$. This U-Net is additionally conditioned on high-level semantic features derived from the text input using a pre-trained Flan-T5-Base language model @chung_scaling_2024. The Mel spectrogram $cal(R)_"MEL" (tilde(S))$ serves as the final acoustic output of the #abbr.a[TTS] system:
    
    $
    cal(R)_"MEL" (tilde(S)) = "U-Net"_"DEC" (P', Z_"SPK", f^"T5"_phi (T)) 
    $
    
    Both U-Net models are trained using either the #abbr.a[MSE] objective or the #abbr.pla[DDPM] objective, allowing for a direct comparison of their scaling behaviors.

#comic.comic((80mm, 40mm), "Simplified TTS model architecture utilizing two U-Net models for prosody representation and Mel spectrogram generation. The U-Net Encoder generates a prosody representation conditioned on phone sequence and speaker identity. The U-Net Decoder produces Mel spectrograms, conditioned on the prosody representation, speaker identity, and text features.", blue) <fig_tts_arch_scaling>

Our datasets are derived from the large-scale LibriHeavy corpus @kang_libriheavy_2024. To ensure controlled comparisons, we create distinct subsets varying along two dimensions:
#emph[Dataset Size]: The size of the #abbr.a[TTS] training data is varied across several points: 100 hours, 300 hours, 500 hours, 1000 hours, 2500 hours, and 5000 hours.
#emph[Speaker Diversity]: For each dataset size, we create subsets with varying levels of speaker diversity. The 'low' diversity setting includes 25 speakers for the 100-hour dataset, scaling up to 153 speakers for the 5000-hour dataset. The 'medium' diversity setting includes 40 speakers for 100 hours, scaling to 956 speakers for 5000 hours. The 'high' diversity setting starts at 62 speakers for 100 hours, scaling to 1531 speakers for 5000 hours. This controlled variation allows us to analyze the interaction between dataset size and speaker count.

Within each experiment, three distinct subsets are created: one for #abbr.a[TTS] training, one for #abbr.a[ASR] training (using the synthetic output from the #abbr.a[TTS] model), and one for #abbr.a[ASR] evaluation (using real data). Crucially, there is no overlap in transcripts between these three sets, ensuring that #abbr.a[ASR] performance differences are solely attributable to variations in the #abbr.a[TTS] models and dataset conditions, not to data leakage or evaluation bias (see @05_setup for details). The proportions of speakers are consistently maintained across all three subsets to ensure consistency in speaker representation.

The #abbr.a[TTS] models (both #abbr.a[MSE] and #abbr.a[DDPM] configurations) are trained for 500,000 iterations using a batch size of 16 and a cosine learning rate schedule starting at $4 times 10^(-5)$. To stabilize training, an #abbr.a[EMA] decay rate of 0.9999 is applied to the model parameters. For inference with #abbr.pla[DDPM] models, a DDIM sampler is employed with 20 sampling steps, utilizing a classifier-free guidance weight of 7.5 and a rescale factor of 0.7 for optimal control over generation quality. In line with prior research @lin_common_2024, the training utilizes a rescaled noise schedule to ensure zero terminal #abbr.a[SNR] and inference commences from the last timestep, ensuring congruency between training and inference processes. Additionally, during training, a random uniform masking of 0-50% of the input phone sequence is applied. This technique helps prevent the model from overfitting to local characteristics of the phone sequence, encouraging it to learn more robust and generalized acoustic-to-text mappings.

For #abbr.a[ASR] evaluation, both the previously introduced hybrid HMM-TDNN model (see @05_hybrid) and Conformer-CTC model are utilized. The primary metric for assessing the quality of the synthetic data is the #abbr.s[WERR], defined as the ratio of the #abbr.a[WER] achieved by an #abbr.a[ASR] model trained exclusively on synthetic speech to the #abbr.a[WER] achieved by the same #abbr.a[ASR] model trained exclusively on real speech.

=== Observed Scaling Behaviour

Our experiments reveal distinct scaling behaviors for #abbr.a[MSE]-based and #abbr.pla[DDPM]-based #abbr.a[TTS] models when used for #abbr.a[ASR] training. These findings are summarized in @tab_scaling_results, which presents #abbr.s[WERR] values across varying #abbr.a[TTS] training dataset sizes and speaker diversity levels.

The #abbr.a[MSE] model consistently demonstrated stronger performance in low-data regimes, specifically up to approximately 100 hours of #abbr.a[TTS] training data, regardless of the level of speaker diversity. For example, at 100 hours, the #abbr.a[MSE] model achieved #abbr.s[WERR] values of 3.62 for low speaker diversity, 3.66 for medium diversity, and 3.92 for high diversity. In contrast, the #abbr.pla[DDPM] model under the same conditions yielded #abbr.s[WERR] values of 8.07, 8.33, and 7.44 respectively. This initial advantage for #abbr.a[MSE] models can be attributed to their more deterministic nature, which may be beneficial when the training data is scarce and the model needs to quickly learn basic speech generation patterns without excessive stochasticity @pine_requirements_2022. However, a limitation of the #abbr.a[MSE] model becomes evident with increasing dataset size. It shows very limited improvement beyond the initial gains, reaching a plateau. This aligns with the hypothesis that #abbr.a[MSE]-based models are intrinsically biased towards generating oversmoothed outputs, thereby failing to leverage the richer variability present in larger datasets -- visual spot checking confirms this, with Mel spectrograms generated by the MSE model having a "blurry" quality #ac("TODO: add figure"). This suggests that further scaling of #abbr.a[MSE] models offers diminishing returns, making them less suitable for scenarios requiring large amounts of diverse training data.

In contrasting this, the #abbr.pla[DDPM] model exhibits different scaling behavior. While initially underperforming in smaller data regimes (approximately 300 hours and less), the #abbr.pla[DDPM] model shows significant and continuous improvements as the #abbr.a[TTS] training dataset size increases. It closes the gap with #abbr.a[MSE] models and eventually outperforms them in larger data regimes. For instance, at 2500 hours, the #abbr.pla[DDPM] model achieves the best reported #abbr.s[WERR] of 1.46 for high speaker diversity, substantially lower than the #abbr.a[MSE] model's 2.59 #abbr.s[WERR] at the same scale and diversity. This indicates that the #abbr.pla[DDPM] model's inherent stochasticity and its ability to model complex, multi-modal distributions allow it to make more effective use of larger and more diverse datasets, translating into better #abbr.a[ASR] performance.

Speaker diversity further shows an increased advantage to the #abbr.pla[DDPM] models. While #abbr.a[MSE] models showed inconsistent benefits from increased speaker diversity (sometimes even degrading performance with more speakers), #abbr.pla[DDPM] models consistently performed better with higher speaker diversity at larger dataset sizes. At 5000 hours, the highest speaker diversity setting yielded a 4% better #abbr.s[WERR] compared to the lowest diversity setting for #abbr.pla[DDPM]. However, it is important to note that even for #abbr.pla[DDPM] models, the relative difference between the lowest and highest diversity settings diminished as training data size increased (from 8% at 100 hours to 4% at 5000 hours). This suggests that a similar effect of diminishing returns, akin to that observed with overall dataset size, also applies to speaker diversity.

#let werr_value(wer_synth, wer_real) = calc.round(wer_synth / wer_real, digits: 2)
#let wer_cell(wer, std_dev) = "$#wer $ ± $#std_dev$"

#figure(
  [placeholder],
  caption: [Results for different dataset sizes and speaker diversities. WER values have been converted from percentages to raw values for consistency with WERR.],
) <tab_scaling_results>

@tab_scaling_results presents the detailed #abbr.s[WERR] and #abbr.a[WER] results across varying #abbr.a[TTS] training dataset sizes and speaker diversity levels. The trends illustrate the distinct scaling behaviors of #abbr.pla[DDPM] and #abbr.a[MSE] models.

=== Proposed Scaling Law

To quantitatively model the observed scaling dynamics in #abbr.a[TTS]-for-#abbr.a[ASR], we propose a two-term power law that accounts for two distinct phases in performance improvement. This framework aims to capture both the initial rapid gains in low-data regimes and the eventual diminishing returns as dataset size increases, which are often indicative of underlying model limitations. The proposed scaling law for #abbr.s[WERR] as a function of dataset size ($D$) is given by:
$ text("WERR")(D) prop D^(-alpha) + D^(-gamma) $
Here, $alpha$ and $gamma$ are positive constants. The term $D^{-alpha}$ parametrizes the initial variance-limited phase, where additional data leads to rapid improvements in #abbr.a[ASR] performance as the #abbr.a[TTS] model learns to better approximate the underlying speech distribution and leverage the variability present in larger datasets. The term $D^{-gamma}$ parametrizes the resolution-limited phase, representing the diminishing returns observed as the dataset size further increases. In this phase, the #abbr.a[TTS] model's complexity or inherent biases (such as oversmoothed outputs when trained using #abbr.a[MSE]) limit its ability to extract further usefulness from additional synthetic data for #abbr.a[ASR].

By fitting this two-term power law to our experimental results, we derived approximate values for these parameters for both model types:
   For the #abbr.pla[DDPM] model, the fitted parameters were $alpha = 1.86$ and $gamma = 0.06$. The smaller $alpha$ value suggests a slower initial rate of improvement in the variance-limited phase compared to #abbr.a[MSE]. However, the larger $gamma$ value indicates that #abbr.pla[DDPM] exhibits a more sustained scaling behavior in the resolution-limited phase, leading to continuous, albeit slower, improvements with very large datasets.
   For the #abbr.a[MSE] model, the fitted parameters were $alpha = 2.93$ and $gamma = 0.01$. The larger $alpha$ value for #abbr.a[MSE] reflects a faster initial rate of improvement in the variance-limited phase, indicating that #abbr.a[MSE] models quickly learn the core speech generation patterns in low-data settings. However, the significantly smaller $gamma$ value (closer to zero) indicates that #abbr.a[MSE] models experience a much quicker stagnation in #abbr.s[WERR] as dataset size increases, confirming their earlier plateauing behavior observed in the results section.

These fitted power laws are visually represented in @fig_werr_scaling. Even with the improved scaling properties of #abbr.pla[DDPM], our optimistic projections indicate that an enormous amount of synthetic data—on the order of at least one million hours—would be required for #abbr.pla[DDPM]-generated speech to match the performance of #abbr.a[ASR] systems trained on real data. This requirement far exceeds the size of currently available open datasets for #abbr.a[TTS] training @pratap_mls_2020@chen_gigaspeech_2021, highlighting the substantial persistent gap that remains.

#comic.comic((80mm, 40mm), "Word Error Rate Ratio (WERR) as a function of #abbr.a[TTS] training dataset size for #abbr.pla[DDPM] and #abbr.a[MSE] models. This conceptual plot illustrates the distinct scaling behaviors: #abbr.pla[DDPM] shows better scalability over large datasets, while #abbr.a[MSE] plateaus due to oversmoothing.", blue) <fig_werr_scaling>

@fig_werr_scaling visually depicts the distinct scaling behaviors. It shows that while #abbr.a[MSE] models quickly reach a performance plateau, #abbr.pla[DDPM] models continue to improve with increasing data, albeit at a diminishing rate.

=== Discussion

These findings provide crucial insights into the effectiveness of different #abbr.a[TTS] training objectives for generating synthetic speech suitable for #abbr.a[ASR] training. The results indicate that the inherent limitations of #abbr.a[MSE]-based models, primarily stemming from their unimodal assumption and tendency towards oversmoothing, severely constrain their scalability and ability to leverage large, diverse datasets. This leads to a rapid plateau in #abbr.a[ASR] performance, making them less suitable for scenarios where vast amounts of synthetic data are desirable.

Conversely, #abbr.pla[DDPM] models, with their probabilistic nature and capability to model complex, multi-modal distributions, demonstrate significantly better scalability. They are more effective at utilizing large and diverse #abbr.a[TTS] training datasets, leading to sustained improvements in #abbr.a[ASR] performance. This makes #abbr.pla[DDPM]-based #abbr.a[TTS] models a more promising direction for future large-scale speech synthesis applications, particularly as access to massive unlabeled audio corpora continues to grow.

However, despite #abbr.pla[DDPM]'s superior scaling properties, our proposed two-term power law reveals that diminishing returns persist. Even with optimistic assumptions about the scaling behavior, reaching a #abbr.s[WERR] comparable to that achieved with real speech would require an unfeasibly large amount of synthetic data, potentially millions of hours, far exceeding the size of currently available open datasets. This indicates that while scaling is a powerful strategy, it alone may not be sufficient to fully bridge the performance gap between synthetic and real speech for #abbr.a[ASR] training. The persistent gap suggests that intrinsic limitations of current #abbr.a[TTS] models or the fundamental differences between synthetic and real speech distributions — such as unmodeled nuances of natural human variability or subtle artifacts — remain.

This observation highlights the necessity of exploring alternative and complementary approaches beyond merely scaling synthetic data. This persistent gap also underscores the critical need for robust and objective evaluation methodologies, like the #abbr.s[WERR] and other distributional distance metrics introduced in @05_ttsasr[Chapter 5] and @01_intro[Chapter 1], to accurately quantify progress and identify remaining challenges in creating truly human-like and functionally effective synthetic speech. The subsequent chapters of this thesis will delve further into such evaluation methodologies, emphasizing their role in guiding research towards closing this crucial gap.