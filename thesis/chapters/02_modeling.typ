#import "../abbr.typ" 
#import "@preview/drafting:0.2.2": inline-note

== Modeling and Training TTS and ASR Models <02_modeling>

In this chapter, we introduce common architectures and training approaches #abbr.l[TTS] and #abbr.l[ASR] as preliminaries for our contributions in later chapters. They are introduced here since TTS-for-ASR requires an understanding of both (@03_ttsasr[Chapter]).

=== #abbr.l[TTS]

As outlined in @01_intro[Chapter], we constrain this work to multi-speaker voice-cloning #abbr.a[TTS], in which there are two inputs; a speaker representation derived from a reference recording, which is most commonly a speaker embedding (see @06_speaker), but could also be a Mel Spectrogram or any other representation containing information about the given speaker @eskimez_e2_2024, like a text prompt describing their characteristics @lyth_parler_2024. Mapping these inputs to an acoustic realisation is a complex "one-to-many" problem @ren_revisiting_2022@blumstein_phonetic_1981.
There are two main paradigms for accomplishing this task:

==== Hierarchical

Seeing #abbr.a[TTS] as a *hierarchical pipeline* breaks the problem into a series of steps and representations, moving from the utterance-level information such as speaker @stanton_speaker_2022 and lexical content, to phone level, frame level (i.e. mel spectrogram or MFCC frames) to sample level. The precise levels might differ in their definition and purpose between systems, but generally there is a gradual transformation from lower- to higher-resolution representations, ending in the raw waveform. The individual transformations might be accomplished using #abbr.pla[DNN], other learned modules or rule-based systems.

#inline-note[Expand this and add a figure, the hierarchical approach is a good way to explain TTS in general.]

==== #abbr.l[E2E]
The second paradigm is the #abbr.a[E2E] approach in which a #abbr.a[DNN] directly predicts the output from the input. In many other domains, this approach has lead to consistently better results, however, for the high-resolution, continuous and highly inter-correlated nature of audio signals, this does not necessarily seem to be the case at the time of writing. Even the most recent #abbr.a[E2E] systems often use one or two components of the hierarchical approach, most commonly the #emph[vocoder], which converts mel spectrograms or other intermediate representations to raw waveforms @eskimez_e2_2024@chen_vall-e_2024, as well as the #abbr.a[g2p] conversion module @casanova_xtts_2024.

==== #abbr.l[AR] and #abbr.l[NAR]

If we let $T$ be the input lexical sequence (e.g., sub-word tokens), $bold(S) = (s_1, dots, s_n)$ be the target acoustic sequence (e.g., Mel-spectrogram frames), and $bold(e)_S$ be the speaker embedding vector for the target speaker, the goal of a multi-speaker conditional TTS model is to learn the distribution $p(bold(S)|bold(T),bold(e)_S)$.

*#abbr.l[AR] models* model this probability sequentially such that:

$ p(bold(S)|bold(T),bold(e)_S) = product_(i=1)^n p(s_i|s_1,dots,s_(i-1),bold(T),bold(e)_S) $

#abbr.a[AR] can lead to better #abbr.a[TTS] performance, but usually shows less strict adherence to the lexical and speaker conditioning, since it is additionally conditioned on its own output, which can cause it to revert to unconditional generation in some cases @mehta_neuralhmm_2022.

In contrast *#abbr.l[NAR] models* assume conditional independence between output frames given the full conditioning information:

// p(\mathbf{S}|\mathbf{T}, \mathbf{e}_{spk}) = \prod_{i=1}^{n} p(s_i | \mathbf{T}, \mathbf{e}_{spk})
$ p(bold(S)|bold(T),bold(e)_S) = product_(i=1)^n p(s_i|bold(T),bold(e)_S) $


==== Objectives

When #link(<part_01>, [Part I]) of this work was conceptualised, the most common training objective for TTS was the #abbr.a[MSE] loss in use for the #abbr.a[NAR] FastPitch@lancucki_fastpitch_2021 as well as FastSpeech 1 @ren_fastspeech_2019 and 2 @ren_fastspeech_2021. On the #abbr.a[AR], Tacotron2 @wang_tacotron_2017 uses the same objective.

$ cal(L)_(text("MSE"))(theta)=EE[||bold(S)-f(bold(T),bold(e)_S;theta)||_2^2] $

Our exploration of different forms of conditioning for TTS-for-ASR systems in @04_attr[Chapter] utilises this objective. 

However, this can lead to oversmoothing of the output @ren_revisiting_2022 which caused the exploration of other objectives. Among these is the *#abbr.a[DDPM] objective* @ho_denoising_2020 which we explore in detail in @05_scaling[Chapter]. In this approach speech is generated by learning to reverse a fixed forward process, $q$, which gradually adds Gaussian noise to the target speech $bold(S)$ over $N$ steps. This noising process is governed by a variance schedule $beta_n$. The model, parameterized by $theta$, learns the reverse denoising process, $p_theta(s_(n-1)|s_n)$, by predicting the parameters $(mu_theta, sigma_theta)$ to remove the noise at each step. It is trained by minimizing a loss function derived from the evidence lower bound (ELBO), which ensures the model can accurately reconstruct the original data from a noised state.

The simplified objective is to train a noise-prediction network, $epsilon_theta$, to predict the added noise $epsilon$ from a noised sample $bold(S)_t$ at any timestep $t$:

$ cal(L)_(text("DDPM"))(theta) = EE_(bold(S)_0, bold(epsilon), t, c) [|bold(epsilon) - bold(epsilon)_theta (bold(S)_t, t, c)\|_2^2] $

where $c$ represents the conditioning variables such as text and speaker information.

#pagebreak()

=== #abbr.l[ASR]

Automatic Speech Recognition (ASR) systems perform the inverse task of TTS, mapping an acoustic signal to its corresponding lexical transcription. Modern high-performance ASR systems are almost exclusively trained using discriminative objectives. In contrast to generative approaches that might model the probability of an acoustic sequence given a text, discriminative models are optimized to directly model the posterior probability $p(bold(T)|bold(S))$, maximizing the score of the correct transcription while simultaneously minimizing the scores of all incorrect competing hypotheses. This approach has proven more effective at achieving low Word Error Rates (WER). The two dominant paradigms for discriminative ASR training are hybrid HMM-DNN systems and end-to-end models.

==== Hybrid HMM-DNN Systems

Hybrid systems combine the strengths of Deep Neural Networks (DNNs) for acoustic modeling with the temporal modeling capabilities of Hidden Markov Models (HMMs). In this architecture, the DNN, often a Time-Delay Neural Network (TDNN) @peddinti_time_2015, processes acoustic feature vectors $bold(o)_t$ at each time step $t$ and outputs a posterior probability distribution over the set of HMM states $q$:

$ p(q_t | bold(o)_t; theta) $

While this acoustic model can be trained with a simple cross-entropy loss against frame-level alignments, system performance is substantially improved through sequence-level discriminative training. The state-of-the-art objective for this is #abbr.a[LF-MMI] @povey_purely_2016. The MMI criterion aims to maximize the mutual information between the observation sequence $bold(O)$ and the reference word sequence $W_text("ref")$. This is achieved by maximizing the log-likelihood of the correct transcription (numerator) while minimizing the log-likelihood of all possible transcriptions (denominator):

$ cal(L)_(text("MMI"))(theta) = log (p_theta(bold(O) | cal(M)_(W_text("ref"))) p(W_text("ref"))) / (sum_W p_theta(bold(O) | cal(M)_W) p(W)) $

Here, $cal(M)_W$ is the HMM corresponding to a word sequence $W$, and $p(W)$ is a language model probability. The numerator represents the likelihood of the correct transcription, while the denominator sums over the likelihoods of all possible transcriptions, explicitly creating a margin that pushes down the scores of incorrect hypotheses. The "Lattice-Free" component streamlines this process by using a simpler phone-level decoding graph, allowing for more efficient end-to-end discriminative training.

==== End-to-End Models

#abbr.a[E2E] models represent a paradigm shift, collapsing the entire ASR pipeline into a single, deep neural network. Architectures such as the Conformer @gulati_conformer_2020 have become standard, learning a direct mapping from an acoustic sequence $bold(X)$ to a label sequence $bold(Y)$ (e.g., characters or sub-words). The key enabling technology for this is the #abbr.a[CTC] loss function @graves_ctc_2012.

CTC circumvents the need for pre-aligned data by introducing a special "blank" token (â€“) to the output vocabulary. It defines a many-to-one mapping function, $cal(B)$, that first collapses any repeated non-blank labels and then removes all blank tokens from a frame-level output path $bold(pi)$ to yield the final, shorter label sequence $bold(Y)$. The total conditional probability of the target sequence is the sum of probabilities of all alignment paths that map to it:

$ p(bold(Y)|bold(X)) = sum_(bold(pi) in cal(B)^(-1)(bold(Y))) p(bold(pi)|bold(X)) $

The probability of any single path $bold(pi)$ is calculated as the product of the per-timestep softmax outputs from the neural network:

$ p(bold(pi)|bold(X)) = product_(t=1)^T p(pi_t | bold(X)) $

The CTC loss is the negative log-likelihood of this total probability, $cal(L)_("CTC") = -log p(bold(Y)|bold(X))$, which is calculated efficiently using dynamic programming. By maximizing the summed probability of all valid paths, the CTC objective implicitly minimizes the probability of all other label sequences, making it an inherently discriminative training criterion.