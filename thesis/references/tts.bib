@inproceedings{oord_wavenet_2016,
  title={{WaveNet}: A Generative Model for Raw Audio},
  author={van den Oord, A{\"a}ron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  booktitle={SSW},
  year={2016}
}


@article{tan_survey_2021,
  title={A survey on neural speech synthesis},
  author={Tan, Xu and Qin, Tao and Soong, Frank and Liu, Tie-Yan},
  journal={arXiv:2106.15561},
  year={2021}
}

@inproceedings{wu_merlin_2016,
  title={Merlin: An open source neural network speech synthesis system},
  author={Wu, Zhizheng and Watts, Oliver and King, Simon},
  booktitle={SSW},
  year={2016}
}

@inproceedings{yao_g2p_2015,
  title={Sequence-to-sequence neural net models for grapheme-to-phoneme conversion},
  author={Yao, Kaisheng and Zweig, Geoffrey},
  booktitle={Proc. Interspeech 2015},
  pages={3330--3334},
  year={2015}
}


@article{chen_f5_2024,
      title={F5-TTS: A Fairytaler that Fakes Fluent and Faithful Speech with Flow Matching}, 
      author={Yushen Chen and Zhikang Niu and Ziyang Ma and Keqi Deng and Chunhui Wang and Jian Zhao and Kai Yu and Xie Chen},
      journal={arXiv:2410.06885},
      year={2024},
  }

@inproceedings{wang_slmeval_2024,
  title={Evaluating Text-to-Speech Synthesis from a Large Discrete Token-based Speech Language Model},
  author={Wang, Siyang and Sz{\'e}kely, {\'E}va},
  booktitle={Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)},
  year={2024}
}

@article{kong_hifigan_2020,
  title={Hifi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis},
  author={Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
  journal={NeurIPS},
  year={2020}
}


@article{kim_glowtts_2020,
  title={{Glow-TTS}: A generative flow for text-to-speech via monotonic alignment search},
  author={Kim, Jaehyeon and Kim, Sungwon and Kong, Jungil and Yoon, Sungroh},
  journal={NeurIPS},
  year={2020}
}

@InProceedings{kalchbrenner_wavernn_2018,
  title = 	 {Efficient Neural Audio Synthesis},
  author =       {Kalchbrenner, Nal and Elsen, Erich and Simonyan, Karen and Noury, Seb and Casagrande, Norman and Lockhart, Edward and Stimberg, Florian and van den Oord, Aaron and Dieleman, Sander and Kavukcuoglu, Koray},
  booktitle = 	 {ICML},
  year = 	 {2018},
}

@inproceedings{peng_paranet_2020,
  title={Non-autoregressive neural text-to-speech},
  author={Peng, Kainan and Ping, Wei and Song, Zhao and Zhao, Kexin},
  booktitle={ICML},
  year={2020},
}

@inproceedings{arik_deepvoice_2017,
  title={Deep Voice: Real-time neural text-to-speech},
  author={Ar{\i}k, Sercan {\"O} and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew and Raiman, Jonathan and others},
  booktitle={ICML},
  year={2017},
}

@article{gibiansky_deepvoice2_2017,
  title={Deep voice 2: Multi-speaker neural text-to-speech},
  author={Gibiansky, Andrew and Arik, Sercan and Diamos, Gregory and Miller, John and Peng, Kainan and Ping, Wei and Raiman, Jonathan and Zhou, Yanqi},
  journal={NeurIPS},
  year={2017}
}

@inproceedings{ling_hybrid_2007,
  title={{HMM}-based hierarchical unit selection combining Kullback-Leibler divergence with likelihood criterion},
  author={Ling, Zhen-Hua and Wang, Ren-Hua},
  booktitle={ICASSP},
  year={2007},
}

@article{tukoda_hmm_2013,
  author={Tokuda, Keiichi and Nankaku, Yoshihiko and Toda, Tomoki and Zen, Heiga and Yamagishi, Junichi and Oura, Keiichiro},
  journal={Proceedings of the IEEE}, 
  title={Speech Synthesis Based on Hidden Markov Models}, 
  year={2013},
}

@inproceedings{hunt_unit_1996,
  title={Unit selection in a concatenative speech synthesis system using a large speech database},
  author={Hunt, A.J. and Black, A.W.},
  booktitle={ICASSP},
  year={1996},
}

@inproceedings{le_voicebox_2023,
 author = {Le, Matthew and Vyas, Apoorv and Shi, Bowen and Karrer, Brian and Sari, Leda and Moritz, Rashel and Williamson, Mary and Manohar, Vimal and Adi, Yossi and Mahadeokar, Jay and Hsu, Wei-Ning},
 booktitle = {NeurIPS},
 title = {Voicebox: Text-Guided Multilingual Universal Speech Generation at Scale},
 year = {2023}
}

@article{wang_valle_2023,
  title={Neural codec language models are zero-shot text to speech synthesizers},
  author={Wang, Chengyi and Chen, Sanyuan and Wu, Yu and Zhang, Ziqiang and Zhou, Long and Liu, Shujie and Chen, Zhuo and Liu, Yanqing and Wang, Huaming and Li, Jinyu and others},
  journal={arXiv preprint arXiv:2301.02111},
  year={2023}
}

@article{badlani_multilingual_2023,
	title = {Multilingual {Multiaccented} {Multispeaker} {TTS} with {RADTTS}},
	journal = {arXiv:2301.10335},
	author = {Badlani, Rohan and Valle, Rafael and Shih, Kevin J and Santos, João Felipe and Gururani, Siddharth and Catanzaro, Bryan},
	year = {2023},
}

@article{borsos_soundstorm_2023,
  title={Soundstorm: Efficient parallel audio generation},
  author={Borsos, Zal{\'a}n and Sharifi, Matt and Vincent, Damien and Kharitonov, Eugene and Zeghidour, Neil and Tagliasacchi, Marco},
  journal={arXiv:2305.09636},
  year={2023}
}

@inproceedings{du_multi-speaker_2023,
	title = {Multi-{Speaker} {Multi}-{Lingual} {VQTTS} {System} for {LIMMITS} 2023 {Challenge}},
	booktitle = {{ICASSP}},
	author = {Du, Chenpeng and Guo, Yiwei and Shen, Feiyu and Yu, Kai},
	year = {2023},
}

@article{zhang_speak_2023,
	title = {Speak {Foreign} {Languages} with {Your} {Own} {Voice}: {Cross}-{Lingual} {Neural} {Codec} {Language} {Modeling}},
	journal = {arXiv:2303.03926},
	author = {Zhang, Ziqiang and Zhou, Long and Wang, Chengyi and Chen, Sanyuan and Wu, Yu and Liu, Shujie and Chen, Zhuo and Liu, Yanqing and Wang, Huaming and Li, Jinyu and {others}},
	year = {2023},
}

@inproceedings{ren_revisiting_2022,
	title = {Revisiting {Over}-{Smoothness} in {Text} to {Speech}},
	booktitle = {{ACL}},
	author = {Ren, Yi and Tan, Xu and Qin, Tao and Zhao, Zhou and Liu, Tie-Yan},
	year = {2022},
}

@article{ren_fastspeech_2021,
	title = {{FastSpeech} 2: {Fast} and {High}-{Quality} {End}-to-{End} {Text} to {Speech}},
	journal = {ICLR},
	author = {Ren, Yi and Hu, Chenxu and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
	year = {2021},
}

@inproceedings{huang_fastdiff_2022,
	title = {{FastDiff}: {A} {Fast} {Conditional} {Diffusion} {Model} for {High}-{Quality} {Speech} {Synthesis}},
	booktitle = {{IJCAI}},
	author = {Huang, R and Lam, MWY and Wang, J and Su, D and Yu, D and Ren, Y and Zhao, Z},
	year = {2022},
}

@inproceedings{kroger_articulatory_2023,
	title = {Articulatory {Speech} {Synthesis} in the {Context} of {Speech} {Research} and {Speech} {Technology}: {Review} and {Prospect}},
	booktitle = {{ESSV}},
	author = {Kröger, Bernd J.},
	year = {2023},
}

@inproceedings{shen_natural_2018,
	title = {Natural {TTS} synthesis by conditioning wavenet on mel spectrogram predictions},
	booktitle = {{ICASSP}},
	author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerrv-Ryan, Rj and {others}},
	year = {2018},
}

@article{casanova_xtts_2024,
	title = {{XTTS}: a {Massively} {Multilingual} {Zero}-{Shot} {Text}-to-{Speech} {Model}},
	journal = {arXiv:2401.02839},
	author = {Casanova, Edresson and Davis, Kelly and Gölge, Eren and Göknar, Görkem and Gulea, Iulian and Hart, Logan and Aljafari, Aya and Meyer, Joshua and Morais, Reuben and Olayemi, Samuel and {others}},
	year = {2024},
}

@article{liu_maximizing_2019,
	title = {Maximizing mutual information for tacotron},
	journal = {arXiv:1909.01145},
	author = {Liu, Peng and Wu, Xixin and Kang, Shiyin and Li, Guangzhi and Su, Dan and Yu, Dong},
	year = {2019},
}

@article{tan_naturalspeech_2024,
	title = {{NaturalSpeech}: {End}-to-end text-to-speech synthesis with human-level quality},
	journal = {IEEE Trans. Pattern Anal. Mach. Intell.},
	author = {Tan, Xu and Chen, Jiawei and Liu, Haohe and Cong, Jian and Zhang, Chen and Liu, Yanqing and Wang, Xi and Leng, Yichong and Yi, Yuanhao and He, Lei and {others}},
	year = {2024},
}

@article{budzianowski_pheme_2024,
	title = {Pheme: {Efficient} and {Conversational} {Speech} {Generation}},
	journal = {arXiv:2401.02839},
	author = {Budzianowski, Paweł and Sereda, Taras and Cichy, Tomasz and Vulić, Ivan},
	year = {2024},
}

@article{li_styletts_2024,
	title = {{StyleTTS} 2: {Towards} human-level text-to-speech through style diffusion and adversarial training with large speech language models},
	journal = {NeurIPS},
	author = {Li, Yinghao Aaron and Han, Cong and Raghavan, Vinay and Mischler, Gavin and Mesgarani, Nima},
	year = {2024},
}

@article{qin_openvoice_2023,
	title = {{OpenVoice}: {Versatile} {Instant} {Voice} {Cloning}},
	journal = {arXiv:2312.01479},
	author = {Qin, Zengyi and Zhao, Wenliang and Yu, Xumin and Sun, Xin},
	year = {2023},
}

@inproceedings{lancucki_fastpitch_2021,
	title = {Fastpitch: {Parallel} text-to-speech with pitch prediction},
	booktitle = {{ICASSP}},
	author = {Łańcucki, Adrian},
	year = {2021},
}

@article{wang_tacotron_2017,
	title = {Tacotron: {Towards} {End}-to-{End} {Speech} {Synthesis}},
	journal = {INTERSPEECH},
	author = {Wang, Yuxuan and Skerry-Ryan, RJ and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and {others}},
	year = {2017},
}

@article{andreev_hifi_2022,
	title = {{HiFi}++: a unified framework for neural vocoding, bandwidth extension and speech enhancement},
	journal = {arXiv:2203.13086},
	author = {Andreev, Pavel and Alanov, Aibek and Ivanov, Oleg and Vetrov, Dmitry},
	year = {2022},
}

@article{ju_naturalspeech_2024,
	title = {{NaturalSpeech} 3: {Zero}-shot speech synthesis with factorized codec and diffusion models},
	journal = {arXiv:2403.03100},
	author = {Ju, Zeqian and Wang, Yuancheng and Shen, Kai and Tan, Xu and Xin, Detai and Yang, Dongchao and Liu, Yanqing and Leng, Yichong and Song, Kaitao and Tang, Siliang and {others}},
	year = {2024},
}

@inproceedings{casanova_yourtts_2022,
	title = {{YourTTS}: {Towards} zero-shot multi-speaker tts and zero-shot voice conversion for everyone},
	booktitle = {{ICML}},
	author = {Casanova, Edresson and Weber, Julian and Shulby, Christopher D and Junior, Arnaldo Candido and Gölge, Eren and Ponti, Moacir A},
	year = {2022},
}

@inproceedings{binkowski_high_2019,
	title = {High {Fidelity} {Speech} {Synthesis} with {Adversarial} {Networks}},
	booktitle = {{ICLR}},
	author = {Bińkowski, Mikołaj and Donahue, Jeff and Dieleman, Sander and Clark, Aidan and Elsen, Erich and Casagrande, Norman and Cobo, Luis C and Simonyan, Karen},
	year = {2019},
}

@inproceedings{kim_conditional_2021,
	title = {Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech},
	booktitle = {{ICML}},
	author = {Kim, Jaehyeon and Kong, Jungil and Son, Juhee},
	year = {2021},
}

@inproceedings{luo_lightspeech_2021,
	title = {Lightspeech: {Lightweight} and fast text to speech with neural architecture search},
	booktitle = {{ICASSP}},
	author = {Luo, Renqian and Tan, Xu and Wang, Rui and Qin, Tao and Li, Jinzhu and Zhao, Sheng and Chen, Enhong and Liu, Tie-Yan},
	year = {2021},
}

@article{chen_vall-e_2024,
	title = {{VALL}-{E} 2: {Neural} {Codec} {Language} {Models} are {Human} {Parity} {Zero}-{Shot} {Text} to {Speech} {Synthesizers}},
	journal = {arXiv:2406.05370},
	author = {Chen, Sanyuan and Liu, Shujie and Zhou, Long and Liu, Yanqing and Tan, Xu and Li, Jinyu and Zhao, Sheng and Qian, Yao and Wei, Furu},
	year = {2024},
}

@inproceedings{pine_requirements_2022,
	title = {Requirements and {Motivations} of {Low}-{Resource} {Speech} {Synthesis} for {Language} {Revitalization}},
	booktitle = {{ACL}},
	author = {Pine, Aidan and Wells, Dan and Brinklow, Nathan and Littell, Patrick and Richmond, Korin},
	year = {2022},
}

@inproceedings{hayashi_espnet-tts_2020,
	title = {{ESPnet}-{TTS}: {Unified}, reproducible, and integratable open source end-to-end text-to-speech toolkit},
	booktitle = {{ICASSP}},
	author = {Hayashi, Tomoki and Yamamoto, Ryuichi and Inoue, Katsuki and Yoshimura, Takenori and Watanabe, Shinji and Toda, Tomoki and Takeda, Kazuya and Zhang, Yu and Tan, Xu},
	year = {2020},
}

@article{ren_fastspeech_2019,
	title = {{FastSpeech}: {Fast}, robust and controllable text to speech},
	journal = {NeurIPS},
	author = {Ren, Yi and Ruan, Yangjun and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
	year = {2019},
}

@article{lyth_parler_2024,
  title={Natural language guidance of high-fidelity text-to-speech with synthetic annotations},
  author={Lyth, Dan and King, Simon},
  journal={arXiv:2402.01912},
  year={2024}
}

@inproceedings{eskimez_e2_2024,
  title={{E2 TTS}: Embarrassingly easy fully non-autoregressive zero-shot {TTS}},
  author={Eskimez, Sefik Emre and Wang, Xiaofei and Thakker, Manthan and Li, Canrun and Tsai, Chung-Hsien and Xiao, Zhen and Yang, Hemin and Zhu, Zirun and Tang, Min and Tan, Xu and others},
  booktitle={SLT},
  year={2024},
}

@inproceedings{mehta_neuralhmm_2022,
  title={Neural HMMs are all you need (for high-quality attention-free TTS)},
  author={Mehta, Shivam and Sz{\'e}kely, {\'E}va and Beskow, Jonas and Henter, Gustav Eje},
  booktitle={ICASSP},
  year={2022},
}